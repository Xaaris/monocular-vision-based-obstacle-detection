<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.18"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Monocular vision based obstacle detection: Monocular Vision based Collision Avoidance fusing Deep Neural Network with feature recognition algorithms</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Monocular vision based obstacle detection
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.18 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Monocular Vision based Collision Avoidance fusing Deep Neural Network with feature recognition algorithms </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>Retrieving accurate 3D position and velocity information of objects from a monocular video can potentially be used for obstacle detection and collision avoidance. Todo so, a video is processed with the help of a segmentation network called Mask R-CNN which detects the objects and produces segmentation masks. These masks are later on used to track objects across multiple video frames with one of three feature description algorithms: SIFT, SURF and ORB. Experiments show that while the system is not yet real-time capable, it produces fairly accurate data.</p>
<h2><a class="anchor" id="autotoc_md3"></a>
Examples (click on the gifs to get to a higher quality video):</h2>
<p><a href="https://youtu.be/LYG21iKl7QE"><img src="./../images/images/example_1.gif?raw=true" alt="Example video 1" class="inline"/></a> Scenario 1: Static camera with moving objects</p>
<p><a href="https://youtu.be/ayhgmKT8KWM"><img src="./../images/images/example_2.gif?raw=true" alt="Example video 1" class="inline"/></a> Scenario 2: Dynamic camera with moving objects. Significant camera shake poses problems to object tracking. (Material from the <a href="https://motchallenge.net/data/MOT16/">MOT16 dataset</a>)</p>
<p><a href="https://youtu.be/tHlel_Hwfm0"><img src="./../images/images/example_3.gif?raw=true" alt="Example video 1" class="inline"/></a> Scenario 3: Dynamic camera with static objects. The shape of the chair presents challenges to the segmentation network.</p>
<p><a href="https://youtu.be/lFGqx8aciTU"><img src="./../images/images/example_4.gif?raw=true" alt="Example video 4" class="inline"/></a> Scenario 1: Static camera with moving objects (Material from the <a href="http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d">KITTI dataset</a>)</p>
<p>The red arrows are the object's predicted trajectory. The faint green rectangles in the middle of the objects represents the uncertainty of the Kalman Filter at that step. One can see that it shrinks the longer an object is tracked successfully.</p>
<h2><a class="anchor" id="autotoc_md4"></a>
Usage:</h2>
<p>Input parameters can be specified as command line arguments. Example: </p><div class="fragment"><div class="line">data/video/IMG_5823.mov --from 0 --to 2 --matcherType ORB --inputDimensions 640 360 --inputScale 0.1666</div>
</div><!-- fragment --><p>This will analyze the first two seconds from the input video <code>data/video/IMG_5823.mov</code> with the Orb matcher. The inputs dimensions are 640x360 which is 1/6th of the original size.</p>
<h4><a class="anchor" id="autotoc_md5"></a>
Full list of arguments:</h4>
<ul>
<li><code>input</code>: Video file or directory of images to be processed</li>
<li><code>from</code>: From video second or image number (default: 0)</li>
<li><code>to</code>: To video second or image number (default: None, end of the video)</li>
<li><code>inputType</code>: Input type can be a VIDEO or a directory with IMAGEs (default: VIDEO)</li>
<li><code>inputDimensions</code>: Input dimensions for video or image series</li>
<li><code>inputScale</code>: Scale compared to original video (e.g. 0.5) (default: 1)</li>
<li><code>matcherType</code>: Matcher type can be SIFT, SURF or ORB (default: SIFT)</li>
<li><code>cameraType</code>: Camera type can be IPHONE_XR_4K_60, IPHONE_8_PLUS_4K_60 or FL2_14S3C_C (default: IPHONE_XR_4K_60)</li>
<li><code>inputFps</code>: Fps of input video (default: 60)</li>
<li><code>outputFps</code>: Fps of output video (default: 10)</li>
</ul>
<h4><a class="anchor" id="autotoc_md6"></a>
Further documentation:</h4>
<p>Documentation of the code can be viewed by opening <a href="docs/html/index.html">this file</a> in a browser of your choice. It is generated via <a href="http://www.doxygen.nl">Doxygen</a> and can be regenerated by issuing the following command from the main directory:</p>
<div class="fragment"><div class="line">doxygen docs/Doxyfile </div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md7"></a>
Requirements:</h2>
<ul>
<li>This project requires Python 3.7 as it makes use of the new <a href="https://docs.python.org/3/library/dataclasses.html">Data Classes</a> but Keras/Tensorflow do not support python 3.8 yet.</li>
<li>This repository uses <a href="https://git-lfs.github.com">Git LFS</a> to store the large weights files.</li>
<li>The python dependencies are listed in the requirements.txt. In order to use the SURF algorithm, you need to build OpenCV from source with the <code>non-free</code> flag enabled as it is a patented algorithm. </li>
</ul>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.18
</small></address>
</body>
</html>
